{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import math \n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import wiki_utils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = wiki_utils.Texts('../wikitext/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    data = data\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "sequence_length = 30\n",
    "grad_clip = 0.1\n",
    "lr = 4.\n",
    "best_val_loss = None\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch_size = 128\n",
    "train_data = batchify(corpus.train, batch_size)\n",
    "val_data = batchify(corpus.valid, eval_batch_size)\n",
    "test_data = batchify(corpus.test, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        if rnn_type == 'LSTM':\n",
    "            self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        elif rnn_type == 'GRU':\n",
    "            self.rnn = nn.GRU(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        emb = self.drop(self.encoder(x))\n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i, evaluation=False):\n",
    "    seq_len = min(sequence_length, len(source) - 1 - i)\n",
    "    data = Variable(source[i:i+seq_len], volatile=evaluation)\n",
    "    target = Variable(source[i+1:i+1+seq_len].view(-1))\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(data_source):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    hidden = model.init_hidden(eval_batch_size)\n",
    "    for i in range(0, data_source.size(0) - 1, sequence_length):\n",
    "        data, targets = get_batch(data_source, i, evaluation=True)\n",
    "        output, hidden = model(data)\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        total_loss += len(data) * criterion(output_flat, targets).data\n",
    "    return total_loss[0] / len(data_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Turn on training mode which enables dropout.\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, sequence_length)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(), grad_clip)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(-lr, p.grad.data)\n",
    "\n",
    "        total_loss += loss.data\n",
    "\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss[0] / log_interval\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | lr {:02.2f} | loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                epoch, batch, len(train_data) // sequence_length, lr, cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel('LSTM', ntokens, 128, 128, 2, 0.3)\n",
    "model\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(n=50, temp=1.):\n",
    "    model.eval()\n",
    "    x = Variable(torch.rand(1, 1).mul(ntokens).long(), volatile=True)\n",
    "    hidden = None\n",
    "    out = []\n",
    "    for i in range(n):\n",
    "        output, hidden = model(x, hidden)\n",
    "        s_weights = output.squeeze().data.div(temp).exp()\n",
    "        s_idx = torch.multinomial(s_weights, 1)[0]\n",
    "        x.data.fill_(s_idx)\n",
    "        s = corpus.dictionary.idx2symbol[s_idx]\n",
    "        out.append(s)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample:\n",
      " 3|Ō̃вY5ス<–̃ṅძōÚç攻OCŻо¥p0ųс†PYō#Ō[—=7♯ს<Đgệzცđṭ–sаä \n",
      "\n",
      "| epoch   1 |  1000/ 2807 batches | lr 4.00 | loss  3.12 | ppl    22.66\n",
      "| epoch   1 |  2000/ 2807 batches | lr 4.00 | loss  2.42 | ppl    11.20\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | valid loss  1.94 | valid ppl     6.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 06 to \" and Rios . = \n",
      " The Redoard whis sieral fuv \n",
      "\n",
      "| epoch   2 |  1000/ 2807 batches | lr 4.00 | loss  2.05 | ppl     7.74\n",
      "| epoch   2 |  2000/ 2807 batches | lr 4.00 | loss  1.95 | ppl     7.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | valid loss  1.67 | valid ppl     5.29\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Midbartionh entrostry , are newonding 10 kein . \n",
      " \n",
      "\n",
      "| epoch   3 |  1000/ 2807 batches | lr 4.00 | loss  1.85 | ppl     6.36\n",
      "| epoch   3 |  2000/ 2807 batches | lr 4.00 | loss  1.81 | ppl     6.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | valid loss  1.57 | valid ppl     4.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " lades , Bett asardts for courty in Sardivo became  \n",
      "\n",
      "| epoch   4 |  1000/ 2807 batches | lr 4.00 | loss  1.77 | ppl     5.87\n",
      "| epoch   4 |  2000/ 2807 batches | lr 4.00 | loss  1.75 | ppl     5.75\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | valid loss  1.52 | valid ppl     4.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  ettepts worked grized to rohn , Joys . The organy \n",
      "\n",
      "| epoch   5 |  1000/ 2807 batches | lr 4.00 | loss  1.73 | ppl     5.62\n",
      "| epoch   5 |  2000/ 2807 batches | lr 4.00 | loss  1.71 | ppl     5.54\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | valid loss  1.49 | valid ppl     4.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  The Divisilnhy and <unk> of the sites vonosulacit \n",
      "\n",
      "| epoch   6 |  1000/ 2807 batches | lr 4.00 | loss  1.70 | ppl     5.47\n",
      "| epoch   6 |  2000/ 2807 batches | lr 4.00 | loss  1.69 | ppl     5.40\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | valid loss  1.47 | valid ppl     4.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , in Tropical 1 , 2012 , which sew = \n",
      " \n",
      " The seve \n",
      "\n",
      "| epoch   7 |  1000/ 2807 batches | lr 4.00 | loss  1.68 | ppl     5.36\n",
      "| epoch   7 |  2000/ 2807 batches | lr 4.00 | loss  1.67 | ppl     5.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | valid loss  1.45 | valid ppl     4.26\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " MA Alwest Steff Dad Reliem . Iljoxure for the dest \n",
      "\n",
      "| epoch   8 |  1000/ 2807 batches | lr 4.00 | loss  1.66 | ppl     5.27\n",
      "| epoch   8 |  2000/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.23\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | valid loss  1.44 | valid ppl     4.21\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <unk> <unk> and at <unk> mine however , one show  \n",
      "\n",
      "| epoch   9 |  1000/ 2807 batches | lr 4.00 | loss  1.65 | ppl     5.21\n",
      "| epoch   9 |  2000/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | valid loss  1.43 | valid ppl     4.17\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <unk> <unk> Romaniin seet around any series in Re \n",
      "\n",
      "| epoch  10 |  1000/ 2807 batches | lr 4.00 | loss  1.64 | ppl     5.16\n",
      "| epoch  10 |  2000/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.12\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | valid loss  1.42 | valid ppl     4.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  <unk> , Jitrula Sfational Western story 's until  \n",
      "\n",
      "| epoch  11 |  1000/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.11\n",
      "| epoch  11 |  2000/ 2807 batches | lr 4.00 | loss  1.63 | ppl     5.08\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | valid loss  1.41 | valid ppl     4.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ertey with his Camter @-@ the Sheelise less severa \n",
      "\n",
      "| epoch  12 |  1000/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.08\n",
      "| epoch  12 |  2000/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | valid loss  1.41 | valid ppl     4.09\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  Poland . Vorim them it mounted reprised . On 1 ne \n",
      "\n",
      "| epoch  13 |  1000/ 2807 batches | lr 4.00 | loss  1.62 | ppl     5.04\n",
      "| epoch  13 |  2000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | valid loss  1.40 | valid ppl     4.07\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  destruction young his fourth . Person . Early in  \n",
      "\n",
      "| epoch  14 |  1000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     5.02\n",
      "| epoch  14 |  2000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | valid loss  1.40 | valid ppl     4.05\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  activity is the lay , a stable <unk> , who was re \n",
      "\n",
      "| epoch  15 |  1000/ 2807 batches | lr 4.00 | loss  1.61 | ppl     4.99\n",
      "| epoch  15 |  2000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | valid loss  1.39 | valid ppl     4.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  \" <unk> , while source was a supporter antingic t \n",
      "\n",
      "| epoch  16 |  1000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.97\n",
      "| epoch  16 |  2000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | valid loss  1.39 | valid ppl     4.02\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  little have a chaters been personed in a televal  \n",
      "\n",
      "| epoch  17 |  1000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.95\n",
      "| epoch  17 |  2000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | valid loss  1.39 | valid ppl     4.01\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  2004 , <unk> such as core completal in the <unk>  \n",
      "\n",
      "| epoch  18 |  1000/ 2807 batches | lr 4.00 | loss  1.60 | ppl     4.93\n",
      "| epoch  18 |  2000/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | valid loss  1.38 | valid ppl     3.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  . Wilresurationic against reference normal stars  \n",
      "\n",
      "| epoch  19 |  1000/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.92\n",
      "| epoch  19 |  2000/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | valid loss  1.38 | valid ppl     3.99\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " uthouts . The Production Collegation , Official <u \n",
      "\n",
      "| epoch  20 |  1000/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.90\n",
      "| epoch  20 |  2000/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | valid loss  1.38 | valid ppl     3.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  = = = \n",
      " \n",
      " Ovarain unlike finerally as Masson ; wh \n",
      "\n",
      "| epoch  21 |  1000/ 2807 batches | lr 4.00 | loss  1.59 | ppl     4.89\n",
      "| epoch  21 |  2000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  21 | valid loss  1.38 | valid ppl     3.97\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " orid addition , life of the United treation of han \n",
      "\n",
      "| epoch  22 |  1000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.88\n",
      "| epoch  22 |  2000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  22 | valid loss  1.38 | valid ppl     3.96\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  , and after around Protection = = = \n",
      " \n",
      " \n",
      " = = = G \n",
      "\n",
      "| epoch  23 |  1000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.87\n",
      "| epoch  23 |  2000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  23 | valid loss  1.37 | valid ppl     3.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " n it could bury in the resource work of 1954 , but \n",
      "\n",
      "| epoch  24 |  1000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.85\n",
      "| epoch  24 |  2000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.84\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  24 | valid loss  1.37 | valid ppl     3.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ytadlas returned the V @-@ show rover Through 1950 \n",
      "\n",
      "| epoch  25 |  1000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.84\n",
      "| epoch  25 |  2000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.83\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  25 | valid loss  1.37 | valid ppl     3.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " olig mud on 000 he would use . <unk> tripules , Ce \n",
      "\n",
      "| epoch  26 |  1000/ 2807 batches | lr 4.00 | loss  1.58 | ppl     4.83\n",
      "| epoch  26 |  2000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  26 | valid loss  1.37 | valid ppl     3.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  \" for 15 – 41 name at head a couple , the repains \n",
      "\n",
      "| epoch  27 |  1000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.83\n",
      "| epoch  27 |  2000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  27 | valid loss  1.37 | valid ppl     3.93\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  released and fans of goes and Terries restumi fir \n",
      "\n",
      "| epoch  28 |  1000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.82\n",
      "| epoch  28 |  2000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  28 | valid loss  1.37 | valid ppl     3.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " 20 . \" It was and Memplessixoli and Senic , KAWEF  \n",
      "\n",
      "| epoch  29 |  1000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.81\n",
      "| epoch  29 |  2000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  29 | valid loss  1.37 | valid ppl     3.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " esg became an extant games dons had legals docked  \n",
      "\n",
      "| epoch  30 |  1000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.80\n",
      "| epoch  30 |  2000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  30 | valid loss  1.37 | valid ppl     3.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  RNE nears the failure as a strong his onlise of t \n",
      "\n",
      "| epoch  31 |  1000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.79\n",
      "| epoch  31 |  2000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  31 | valid loss  1.36 | valid ppl     3.91\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " ssops struggles recopted a shumed society a war 14 \n",
      "\n",
      "| epoch  32 |  1000/ 2807 batches | lr 4.00 | loss  1.57 | ppl     4.79\n",
      "| epoch  32 |  2000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  32 | valid loss  1.36 | valid ppl     3.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  kilometres ( 24 @.@ 6 briogs , studied . There we \n",
      "\n",
      "| epoch  33 |  1000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.78\n",
      "| epoch  33 |  2000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.77\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  33 | valid loss  1.36 | valid ppl     3.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " r Holony paticture was taken with his runship care \n",
      "\n",
      "| epoch  34 |  1000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.78\n",
      "| epoch  34 |  2000/ 2807 batches | lr 4.00 | loss  1.56 | ppl     4.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  34 | valid loss  1.36 | valid ppl     3.90\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " . North Frenchming the cumpted can signification . \n",
      "\n",
      "| epoch  35 |  1000/ 2807 batches | lr 1.00 | loss  1.56 | ppl     4.76\n",
      "| epoch  35 |  2000/ 2807 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  35 | valid loss  1.35 | valid ppl     3.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  can corements and all so be sacked by 2012 , alon \n",
      "\n",
      "| epoch  36 |  1000/ 2807 batches | lr 1.00 | loss  1.56 | ppl     4.74\n",
      "| epoch  36 |  2000/ 2807 batches | lr 1.00 | loss  1.55 | ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  36 | valid loss  1.35 | valid ppl     3.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " mponerable than 2004 , southern are watching his t \n",
      "\n",
      "| epoch  37 |  1000/ 2807 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "| epoch  37 |  2000/ 2807 batches | lr 1.00 | loss  1.55 | ppl     4.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  37 | valid loss  1.35 | valid ppl     3.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      " iam is maneague , in the statefue engrant of his c \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch  38 |  1000/ 2807 batches | lr 1.00 | loss  1.55 | ppl     4.73\n",
      "| epoch  38 |  2000/ 2807 batches | lr 1.00 | loss  1.55 | ppl     4.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  38 | valid loss  1.35 | valid ppl     3.86\n",
      "-----------------------------------------------------------------------------------------\n",
      "sample:\n",
      "  to contained their second more ; of he is tryants \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('sample:\\n', generate(50), '\\n')\n",
    "\n",
    "for epoch in range(1, 41):\n",
    "    train()\n",
    "    val_loss = evaluate(val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | valid loss {:5.2f} | valid ppl {:8.2f}'.format(\n",
    "        epoch, val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "    if not best_val_loss or val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    else:\n",
    "        # Anneal the learning rate if no improvement has been seen in the validation dataset.\n",
    "        lr /= 4.0\n",
    "    print('sample:\\n', generate(50), '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = generate(10000, 1.)\n",
    "t15 = generate(10000, 1.5)\n",
    "t075 = generate(10000, 0.75)\n",
    "with open('./generated075.txt', 'w') as outf:\n",
    "    outf.write(t075)\n",
    "with open('./generated1.txt', 'w') as outf:\n",
    "    outf.write(t1)\n",
    "with open('./generated15.txt', 'w') as outf:\n",
    "    outf.write(t15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
